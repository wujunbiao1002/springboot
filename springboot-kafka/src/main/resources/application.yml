server:
  port: 8088
spring:
  application:
    name: springboot-kafka
  kafka:
    bootstrap-servers: 192.168.80.101:9092,192.168.80.102:9092,192.168.80.103:9092
    producer:
#      key-serializer: org.apache.kafka.common.serialization.StringSerializer
#      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      retries: 2 # 重试次数
#      buffer-memory: 5242880 # 生产端缓冲区大小 33554432B=32M | 5242880=5M
      acks: 1 # 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      group-id: ${spring.application.name}-zabbix_group
#      enable-auto-commit: false # 是否自动提交offset
#      auto-commit-interval: 1000  # 提交offset延时(接收到消息后多久提交offset)
      auto-offset-reset: latest # 当kafka中没有初始offset或offset超出范围时将自动重置offset, earliest:重置为分区中最小的offset; latest:重置为分区中最新的offset(消费分区中新产生的数据); none:只要有一个分区不存在已提交的offset,就抛出异常;
#      properties:
#        session.timeout.ms: 120000 # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
#        request.timeout.ms: 120000 # 消费请求超时时间
      max-poll-records: 10 # 批量消费每次最多消费多少条消息
    listener:
      missing-topics-fatal: false # 消费端监听的topic不存在时，项目启动会报错(关掉)
      type: batch # 设置批量消费
kafka:
  topic1: springboot_topic1
  topic2: springboot_topic2
